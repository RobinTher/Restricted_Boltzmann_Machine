import numpy as np
import torch
import gc
from Restricted_Boltzmann_machine_training import RBM

import matplotlib.pyplot as plt

plt.rcParams["font.family"] = "serif"
plt.rcParams["mathtext.fontset"] = "dejavuserif"

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

'''
beta = 1.2
# beta = 2.8

n_alpha = 20
alpha_range = np.linspace(0.1, 2, num = n_alpha, endpoint = True)
# alpha_range = np.linspace(0.1, 1, num = n_alpha, endpoint = True)

N = 512
P = 1
P_t = 2
m_0 = 0.2

# It is very fast to sample from the teacher
number_teacher_sampling_steps = 100
number_monitored_sampling_steps = 10

number_student_training_epochs = 36*12000
number_monitored_training_epochs = 20
number_burn_in_epochs = 108000
number_magnetization_samples = 1000

random_number_seed = 37

random_batch_seed = 87

field_magnitude = 0.04
# field_magnitude = 0.
'''

def simulation_run_mismatched_P(beta, alpha_range, N, P, P_t, m_0, number_teacher_sampling_steps,
                                number_monitored_sampling_steps, number_student_training_epochs,
                                number_monitored_training_epochs, number_burn_in_epochs,
                                number_magnetization_samples, random_number_seed,
                                random_batch_seed, field_magnitude = 0.):
    '''
    Train two binary student RBMs (A and B) on data generated by a binary teacher RBM
    for a range of loads alpha in order to reproduce Fig. (4).
    The teacher and students have the same number of visible units N.
    The teacher has P hidden units, while the students have P_t hidden units.
    See Restricted_Boltzmann_machine_training.py for the definition of the RBM class
    and the inputs of this function.
    '''
    n_alpha = len(alpha_range)

    random_number_generator = torch.Generator(device = device)
    random_number_generator.manual_seed(random_number_seed)

    random_batch_generator = torch.Generator(device = "cpu")
    random_batch_generator.manual_seed(random_batch_seed)

    teacher = RBM(N, P, device = device, random_number_generator = random_number_generator,
                random_batch_generator = random_batch_generator).to(device)
    xi_s = teacher.initialize_weights(1., xi_0 = None, binarize = True)

    student_A = RBM(N, P_t, device = device, random_number_generator = random_number_generator,
                    random_batch_generator = random_batch_generator, teacher = teacher).to(device)
    student_A.initialize_weights(m_0, xi_0 = xi_s, P = P, binarize = True, bias_corner = False)

    field_A = field_magnitude/N * torch.cat([xi_s, torch.zeros((N, P), device = device)], dim = 1)

    student_B = RBM(N, P_t, device = device, random_number_generator = random_number_generator,
                    random_batch_generator = random_batch_generator, teacher = teacher).to(device)
    student_B.initialize_weights(m_0, xi_0 = xi_s, P = P, binarize = True, bias_corner = True)

    field_B = field_magnitude/N * torch.tile(xi_s, dims = (1, 2))

    m_A_mean_range = np.zeros(n_alpha)
    m_A_std_range = np.zeros(n_alpha)

    m_B_mean_range = np.zeros(n_alpha)
    m_B_std_range = np.zeros(n_alpha)

    for i, alpha in enumerate(alpha_range):
        M = int(alpha*N)
        beta_eff = beta/N**(1/2)
        
        N_batch_size = 1
        
        sigma = torch.nn.Parameter(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)), requires_grad = False)
        
        teacher.gibbs_sample_visible(sigma, beta_eff, number_teacher_sampling_steps, number_monitored_sampling_steps)
        
        m_A_list = student_A.metropolis_training(sigma, field_A, beta_eff, number_student_training_epochs, N_batch_size, number_monitored_training_epochs,
                                                number_burn_in_epochs, number_magnetization_samples, anneal = False)
        
        m_B_list = student_B.metropolis_training(sigma, field_B, beta_eff, number_student_training_epochs, N_batch_size, number_monitored_training_epochs,
                                                number_burn_in_epochs, number_magnetization_samples, anneal = False)
        
        m_A_range = np.max(np.abs(np.array(m_A_list)), axis = -1)
        
        m_B_range = np.max(np.abs(np.array(m_B_list)), axis = -1)
        
        m_A_mean_range[i] = np.mean(m_A_range)
        m_A_std_range[i] = np.std(m_A_range)
        
        m_B_mean_range[i] = np.mean(m_B_range)
        m_B_std_range[i] = np.std(m_B_range)
        
        del sigma

        # Reinitialize weights
        student_A.initialize_weights(m_0, xi_0 = xi_s, P = P, binarize = True, bias_corner = False)
        
        student_B.initialize_weights(m_0, xi_0 = xi_s, P = P, binarize = True, bias_corner = True)

    with open("./Data/simulated_magnetization_P=%d_beta=%.2f.npy" % (P, beta), "wb") as file:
        np.save(file, m_A_mean_range)
        np.save(file, m_A_std_range)
        np.save(file, m_B_mean_range)
        np.save(file, m_B_std_range)

    del teacher
    del student_A
    del student_B

    gc.collect()

'''
beta = 4

n_alpha = 20
alpha_range = np.linspace(0.1, 1, num = n_alpha, endpoint = True)

N = 512
P = 1
P_t = 2
m_0 = 0.2

# It is very fast to sample from the teacher
number_teacher_sampling_steps = 100
number_monitored_sampling_steps = 0

number_student_sampling_steps = 1
initial_learning_rate_A = 0.01
initial_learning_rate_B = 0.012
learning_rate_decay_A = 0.00005
learning_rate_decay_B = 0.005
momentum = 0.8
number_student_training_epochs = 8*7200
number_monitored_training_epochs = 5

number_burn_in_epochs = 6*7200
number_magnetization_samples = 100

random_number_seed = 2

random_batch_seed = 87
'''

def simulation_run_normal_mismatched_P(beta, alpha_range, N, P, P_t, m_0, number_teacher_sampling_steps,
                                       number_monitored_sampling_steps, number_student_sampling_steps,
                                       initial_learning_rate_A, initial_learning_rate_B,
                                       learning_rate_decay_A, learning_rate_decay_B,
                                       momentum, number_student_training_epochs, number_monitored_training_epochs,
                                       number_burn_in_epochs, number_magnetization_samples,
                                       random_number_seed, random_batch_seed):
    '''
    Train two normal student RBMs (A and B) on data generated by a normal teacher RBM
    for a range of loads alpha in order to reproduce Fig. (5).
    The teacher and students have the same number of visible units N.
    The teacher has P hidden units, while the students have P_t hidden units.
    See Restricted_Boltzmann_machine_training.py for the definition of the RBM class
    and the inputs of this function.
    '''
    n_alpha = len(alpha_range)

    monitor_student_sampling = False

    random_number_generator = torch.Generator(device = device)
    random_number_generator.manual_seed(random_number_seed)

    random_batch_generator = torch.Generator(device = "cpu")
    random_batch_generator.manual_seed(random_batch_seed)

    m_A_mean_range = np.zeros(n_alpha)
    m_A_std_range = np.zeros(n_alpha)

    m_B_mean_range = np.zeros(n_alpha)
    m_B_std_range = np.zeros(n_alpha)

    teacher = RBM(N, P, device = device, random_number_generator = random_number_generator,
                random_batch_generator = random_batch_generator).to(device)
    xi_s = teacher.initialize_weights(1., xi_0 = None, binarize = False)

    student_A = RBM(N, P_t, device = device, random_number_generator = random_number_generator,
                    random_batch_generator = random_batch_generator, teacher = teacher).to(device)
    student_A.initialize_weights(0., xi_0 = xi_s, P = P, binarize = False, bias_corner = False)

    student_B = RBM(N, P_t, device = device, random_number_generator = random_number_generator,
                    random_batch_generator = random_batch_generator, teacher = teacher).to(device)
    student_B.initialize_weights(m_0, xi_0 = xi_s, P = P, binarize = False, bias_corner = True)

    # weight_decay = 1 is the theoretically correct value in our setting. The other hyperparameters are set heuristically.
    weight_decay = 1

    for i, alpha in enumerate(alpha_range):
        M = int(alpha*N)
        
        data_batch_size = int(0.5*M)
        
        sigma = torch.nn.Parameter(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)), requires_grad = False)
        
        teacher.gibbs_sample_visible(sigma, beta, number_teacher_sampling_steps, number_monitored_sampling_steps)
        
        loader = torch.utils.data.DataLoader(dataset = sigma, batch_size = data_batch_size, shuffle = True, generator = random_batch_generator)
        
        m_A_list = student_A.langevin_training(loader, beta, alpha, initial_learning_rate_A, learning_rate_decay_A,
                                            weight_decay, momentum, number_student_sampling_steps, number_student_training_epochs,
                                            number_monitored_training_epochs, monitor_student_sampling,
                                            number_burn_in_epochs, number_magnetization_samples)
        
        m_B_list = student_B.langevin_training(loader, beta, alpha, initial_learning_rate_B, learning_rate_decay_B,
                                            weight_decay, momentum, number_student_sampling_steps, number_student_training_epochs,
                                            number_monitored_training_epochs, monitor_student_sampling,
                                            number_burn_in_epochs, number_magnetization_samples)
        
        m_A_range = np.max(np.abs(np.array(m_A_list)), axis = -1)
        
        m_B_range = np.max(np.abs(np.array(m_B_list)), axis = -1)
        
        m_A_mean_range[i] = np.mean(m_A_range)
        m_A_std_range[i] = np.std(m_A_range)
        
        m_B_mean_range[i] = np.mean(m_B_range)
        m_B_std_range[i] = np.std(m_B_range)
        
        del sigma
        
        # Reinitialize weights
        student_A.initialize_weights(0., xi_0 = xi_s, P = P, binarize = False, bias_corner = False)
        
        student_B.initialize_weights(m_0, xi_0 = xi_s, P = P, binarize = False, bias_corner = True)

    with open("./Data/simulated_normal_magnetization_P=%d_beta=%.2f.npy" % (P, beta), "wb") as file:
        np.save(file, m_A_mean_range)
        np.save(file, m_A_std_range)
        np.save(file, m_B_mean_range)
        np.save(file, m_B_std_range)

    del teacher
    del student_A
    del student_B

    gc.collect()

def plot_simulated_overlap_mismatched_P(beta_range, alpha_range, P_sim, P_saddle):
    '''
    Load the magnetization from .npy files written by the function simulation_run_mismatched_P
    and compare it to the prediction of the saddle-point equations in order to reproduce Fig. (4).
    '''
    fig, axes = plt.subplots(nrows = 2, ncols = 2, sharex = False, sharey = "row", figsize = (15, 10))
    near_diagonal_axes, off_diagonal_axes = axes.T
    fig_axis = fig.add_subplot(111, frameon = False)
    plt.tick_params(labelcolor = "none", which = "both", top = False, bottom = False, left = False, right = False)
    
    fontsize = 19
    
    name = "partial_PSB"
    set_header = True
    for beta, alpha, near_diagonal_axis, off_diagonal_axis in zip(beta_range, alpha_range, near_diagonal_axes, off_diagonal_axes):
        with open("./Data/%s_magnetization_P=%d_beta=%.2f.npy" % (name, P_saddle, beta), "rb") as file:
            m_range = np.load(file)
        
        with open("./Data/simulated_magnetization_P=%d_beta=%.2f.npy" % (P_sim, beta), "rb") as file:
            m_A_mean_range = np.load(file)
            m_A_std_range = np.load(file)
            m_B_mean_range = np.load(file)
            m_B_std_range = np.load(file)
        
        near_diagonal_axis.plot(alpha, m_range, color = "C1")
        near_diagonal_axis.errorbar(alpha, m_A_mean_range, m_A_std_range, marker = "o", linestyle = "--", capsize = 3, zorder = 2.5, color = "C0")
        near_diagonal_axis.tick_params(axis = "both", which = "both", labelsize = fontsize)
        if set_header:
            near_diagonal_axis.set_title(r"No PS bias", fontsize = fontsize)
        
        off_diagonal_axis.plot(alpha, m_range, color = "C1")
        off_diagonal_axis.errorbar(alpha, m_B_mean_range, m_B_std_range, marker = "o", linestyle = "--", capsize = 3, zorder = 2.5, color = "C0")
        off_diagonal_axis.tick_params(axis = "both", which = "both", labelsize = fontsize)
        T = 1/beta
        off_diagonal_axis.set_ylabel(r"Temperature $T = %.2f$" % T, fontsize = fontsize, rotation = -90, labelpad = 30)
        off_diagonal_axis.yaxis.set_label_position("right")
        if set_header:
            off_diagonal_axis.set_title(r"PS bias", fontsize = fontsize)
        
        name = "PSB"
        set_header = False
        
    fig_axis.set_xlabel(r"Load $\alpha$", fontsize = fontsize, labelpad = 15)
    fig_axis.set_ylabel(r"Magnetization $m$", fontsize = fontsize, labelpad = 15)
    plt.tight_layout()
    plt.show()

def plot_simulated_normal_overlap_mismatched_P(beta, alpha_range, P_sim, P_saddle):
    '''
    Load the magnetization from .npy files written by the function simulation_run_normal_mismatched_P
    and compare it to the prediction of the saddle-point equations in order to reproduce Fig. (5).
    '''
    fontsize = 13
    
    with open("./Data/partial_PSB_normal_magnetization_P=%d_beta=%.2f.npy" % (P_saddle, beta), "rb") as file:
        m_range = np.load(file)
    
    with open("./Data/simulated_normal_magnetization_P=%d_beta=%.2f.npy" % (P_sim, beta), "rb") as file:
        m_A_mean_range = np.load(file)
        m_A_std_range = np.load(file)
        m_B_mean_range = np.load(file)
        m_B_std_range = np.load(file)
    
    plt.errorbar(alpha_range, m_A_mean_range, m_A_std_range, marker = "o", linestyle = "--", capsize = 3, color = "C0")
    plt.plot(alpha_range, m_range[:, 0], color = "C1")
    plt.errorbar(alpha_range, m_B_mean_range, m_B_std_range, marker = "o", linestyle = "--", capsize = 3, color = "C0")
    plt.plot(alpha_range, m_range[:, 1], color = "C1")
    
    plt.xlabel(r"Load $\alpha$", fontsize = fontsize)
    plt.ylabel(r"Magnetization $m$", fontsize = fontsize)
    plt.xticks(fontsize = fontsize)
    plt.yticks(fontsize = fontsize)
    plt.tight_layout()
    plt.show()